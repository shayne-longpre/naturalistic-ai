{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install datetime\n",
    "%pip install collections\n",
    "%pip install matplotlib.pyplot\n",
    "%pip install seaborn\n",
    "%pip install pyyaml\n",
    "%pip install jsonlines\n",
    "%pip install requests\n",
    "%pip install datasets\n",
    "%pip install torch\n",
    "%pip install scikit-learn\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.helpers import io\n",
    "from src.classes.dataset import Dataset\n",
    "from src.classes.annotation_set import AnnotationSet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset + Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN:\n",
    "PATH_TO_DATASET = \"../data/automatic_annotations_v0/wildchat4k-raw.json\"\n",
    "DATASET_ID = \"wildchat_1m\"\n",
    "PATH_TO_ANNOTATIONS_DIR = \"../data/automatic_annotations_v0/gpto3mini-json-wildchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (w/o annotations)\n",
    "dataset = Dataset.load(PATH_TO_DATASET)\n",
    "\n",
    "# Load annotations into dataset\n",
    "for fpath in io.listdir_nohidden(PATH_TO_ANNOTATIONS_DIR):\n",
    "    annotation_set = AnnotationSet.load_automatic(path=fpath, source=\"automatic_v0\")\n",
    "    dataset.add_annotations(annotation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Research Questions\n",
    "\n",
    "(*) RQ2.1: How does the topic and purpose distribution differ? (brainstorming, right answer, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) RQ2.2: How does style and intent differ?\n",
    "\n",
    "(Anka to look into this; automated annotations/NLP stuff)\n",
    "\n",
    "Crude analysis: Check if prompt contained questionmark; figure out if there's a way to capture MC\n",
    "Meta: What proportion of benchmarks match and don't match? / Lit review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RQ2.3: Are safety benchmarks targeting the same risk factors actually observed in real-world data?\n",
    "\n",
    "Note: Maybe run topic and purpose annotations on evaluation datasets and compare against real-world occurrences?\n",
    "Caveat: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RQ2.4: How does difficulty level differ?\n",
    "\n",
    "Note: For tasks where difficulty is measurable (e.g., questions with difficulty ratings), try to estimate \"difficulty\" of real-world queries via proxy metrics (e.g., question specificity, uncommon domain knowledge required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RQ2.5: How does the user sophistication differ?\n",
    "\n",
    "Note: Indirectly estimate from query phrasing (e.g., jargon usage, nested requests, explicit instructions for style/tone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2.6: How does diversity differ? \n",
    "\n",
    "Note: Maybe something along the lines of lexical diversity, semantic diversity between prompts? Maybe also prompt length, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TBD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
