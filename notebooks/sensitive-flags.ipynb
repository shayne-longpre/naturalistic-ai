{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhipingzhang/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../\")\n",
    "from src.helpers import io\n",
    "from src.classes.dataset import Dataset\n",
    "from src.classes.annotation_set import AnnotationSet\n",
    "from src.helpers.visualisation import (\n",
    "    barplot_distribution,\n",
    "    plot_confusion_matrix,\n",
    "    tabulate_annotation_pair_summary,\n",
    "    analyze_pair_annotations\n",
    ")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions:\n",
    "# 1. What is the distribution of sensitive use flags?\n",
    "# 2. What is the share of conversations flagged for sensitive uses compared to total labeled conversations?\n",
    "# 3. What is the prevalence of sensitive, sexual, or illegal content in user queries?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt-multi_turn_relationship: 0 / 10127 failed due to invalid annotations.\n",
      "prompt-interaction_features: 0 / 10127 failed due to invalid annotations.\n",
      "turn-sensitive_use_flags: 0 / 10127 failed due to invalid annotations.\n",
      "turn-topic: 1 / 10127 failed due to invalid annotations.\n",
      "response-interaction_features: 0 / 10127 failed due to invalid annotations.\n",
      "prompt-function_purpose: 6 / 10127 failed due to invalid annotations.\n",
      "prompt-media_format: 0 / 10127 failed due to invalid annotations.\n",
      "response-media_format: 0 / 10127 failed due to invalid annotations.\n",
      "response-answer_form: 0 / 10127 failed due to invalid annotations.\n"
     ]
    }
   ],
   "source": [
    "# FILL IN:\n",
    "PATH_TO_DATASET = \"../data/static/wildchat4k-raw.json\"\n",
    "DATASET_ID = \"wildchat_1m\"\n",
    "PATH_TO_ANNOTATIONS_DIR = \"../res/gpto3mini-json-wildchat\"\n",
    "\n",
    "# Load dataset (w/o annotations)\n",
    "dataset = Dataset.load(PATH_TO_DATASET)\n",
    "\n",
    "# Load annotations into dataset\n",
    "for fpath in io.listdir_nohidden(PATH_TO_ANNOTATIONS_DIR):\n",
    "    annotation_set = AnnotationSet.load_automatic(path=fpath, source=\"automatic_v0\")\n",
    "    dataset.add_annotations(annotation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question 1: What is the distribution of sensitive use flags?\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of sensitive use flags:\n",
      "None: 9054 instances (88.81%)\n",
      "Sexually explicit content (Other): 523 instances (5.13%)\n",
      "Inciting violence, hateful or other harmful behavior (harassment & bullying): 179 instances (1.76%)\n",
      "Sexually explicit content (fictitious person): 154 instances (1.51%)\n",
      "Inciting violence, hateful or other harmful behavior (physical harm): 78 instances (0.77%)\n",
      "Discriminatory practices (Misrepresentation, stereotyping, or inappropriate reference to sensitive attributes): 62 instances (0.61%)\n",
      "Criminal planning or other suspected illegal activity not listed elsewhere: 31 instances (0.30%)\n",
      "Sexually explicit content (Request/discussion of CSAM): 26 instances (0.26%)\n",
      "Sexually explicit content (real person): 20 instances (0.20%)\n",
      "Privacy concerns (Possible identifiable information): 14 instances (0.14%)\n",
      "Weapons & drugs: 11 instances (0.11%)\n",
      "Cyberattacks (enabling/enacting malware, computer viruses, worms, malicious code, ...): 11 instances (0.11%)\n",
      "Privacy concerns (Possible sensitive information): 10 instances (0.10%)\n",
      "Generating defamatory content: 6 instances (0.06%)\n",
      "Inciting violence, hateful or other harmful behavior (self-harm): 5 instances (0.05%)\n",
      "Possible presence of copyrighted, unreferenced material: 3 instances (0.03%)\n",
      "Impersonation attempts: 3 instances (0.03%)\n",
      "Cyberattacks: 1 instances (0.01%)\n",
      "Privacy concerns (Possible sensitive information) (e.g., API keys, passwords, other confidential information): 1 instances (0.01%)\n",
      "Potential violation of external policy / ethics: 1 instances (0.01%)\n",
      "Criminal planning or other suspected illegal activity not listed: 1 instances (0.01%)\n",
      "Other: 1 instances (0.01%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhipingzhang/Documents/Phd/projects/NaturalisticData/analysis1/naturalistic-ai/notebooks/../src/helpers/visualisation.py:123: UserWarning: constrained_layout not applied because axes sizes collapsed to zero.  Try making figure larger or Axes decorations smaller.\n",
      "  fig.savefig(output_path, bbox_inches='tight')\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "outdir = \"data/annotation_analysis_v0/sensitive_use_flags\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Get distribution of sensitive use flags\n",
    "sensitive_flags_distribution = dataset.get_annotation_distribution(\n",
    "    \"turn_sensitive_use_flags\",\n",
    "    level=\"message\",\n",
    "    annotation_source=\"automatic_v0\",\n",
    "    annotation_as_list_type=False,\n",
    ")\n",
    "\n",
    "# Plot distribution\n",
    "fig = barplot_distribution(\n",
    "    {\"Sensitive Use Flags\": sensitive_flags_distribution}, \n",
    "    normalize=True, \n",
    "    xlabel=\"Flag Type\", \n",
    "    ylabel=\"Proportion\", \n",
    "    title=\"Distribution of Sensitive Use Flags\",\n",
    "    output_path=f\"{outdir}/barchart.png\", \n",
    "    order=\"descending\"\n",
    ")\n",
    "\n",
    "\n",
    "# Display the raw counts and percentages\n",
    "print(\"Distribution of sensitive use flags:\")\n",
    "total_flags = sum(sensitive_flags_distribution.values())\n",
    "for flag, count in sorted(sensitive_flags_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / total_flags) * 100\n",
    "    print(f\"{flag}: {count} instances ({percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question 2: What is the share of conversations with sensitive flags?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversations: 4000\n",
      "Conversations with sensitive flags: 593\n",
      "Percentage of conversations with sensitive flags: 14.82%\n"
     ]
    }
   ],
   "source": [
    "# Count conversations with at least one sensitive flag\n",
    "sensitive_conv_count = 0\n",
    "total_conv_count = len(dataset.data)\n",
    "conv_with_sensitive_flags = {}\n",
    "\n",
    "for conv in dataset.data:\n",
    "    has_sensitive_flag = False\n",
    "    for msg in conv.conversation:\n",
    "        if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "            flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "            \n",
    "            # Handle both list and string cases\n",
    "            if isinstance(flag_value, list):\n",
    "                if any(flag != \"None\" for flag in flag_value):\n",
    "                    has_sensitive_flag = True\n",
    "                    break\n",
    "            elif flag_value and flag_value != \"None\":\n",
    "                has_sensitive_flag = True\n",
    "                break\n",
    "    \n",
    "    if has_sensitive_flag:\n",
    "        sensitive_conv_count += 1\n",
    "        conv_with_sensitive_flags[conv.conversation_id] = True\n",
    "\n",
    "# Calculate percentage\n",
    "percentage_sensitive = (sensitive_conv_count / total_conv_count) * 100\n",
    "\n",
    "print(f\"Total conversations: {total_conv_count}\")\n",
    "print(f\"Conversations with sensitive flags: {sensitive_conv_count}\")\n",
    "print(f\"Percentage of conversations with sensitive flags: {percentage_sensitive:.2f}%\")\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie([sensitive_conv_count, total_conv_count - sensitive_conv_count], \n",
    "        labels=['Sensitive', 'Non-sensitive'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Proportion of Conversations with Sensitive Content')\n",
    "plt.savefig(f\"{outdir}/pie_chart.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question 3: Prevalence of sensitive content in user vs. assistant messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User messages with sensitive flags: 1067/10127 (10.54%)\n",
      "Assistant messages with sensitive flags: 0/10127 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Count sensitive content by role (user vs. assistant)\n",
    "user_sensitive_count = 0\n",
    "assistant_sensitive_count = 0\n",
    "total_user_msgs = 0\n",
    "total_assistant_msgs = 0\n",
    "\n",
    "for conv in dataset.data:\n",
    "    for msg in conv.conversation:\n",
    "        if msg.role == \"user\":\n",
    "            total_user_msgs += 1\n",
    "            if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "                flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "                if isinstance(flag_value, list):\n",
    "                    if any(flag != \"None\" for flag in flag_value):\n",
    "                        user_sensitive_count += 1\n",
    "                elif flag_value and flag_value != \"None\":\n",
    "                    user_sensitive_count += 1\n",
    "        elif msg.role == \"assistant\":\n",
    "            total_assistant_msgs += 1\n",
    "            if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "                flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "                if isinstance(flag_value, list):\n",
    "                    if any(flag != \"None\" for flag in flag_value):\n",
    "                        assistant_sensitive_count += 1\n",
    "                elif flag_value and flag_value != \"None\":\n",
    "                    assistant_sensitive_count += 1\n",
    "\n",
    "# Calculate percentages\n",
    "user_sensitive_percentage = (user_sensitive_count / total_user_msgs) * 100 if total_user_msgs > 0 else 0\n",
    "assistant_sensitive_percentage = (assistant_sensitive_count / total_assistant_msgs) * 100 if total_assistant_msgs > 0 else 0\n",
    "\n",
    "print(f\"User messages with sensitive flags: {user_sensitive_count}/{total_user_msgs} ({user_sensitive_percentage:.2f}%)\")\n",
    "print(f\"Assistant messages with sensitive flags: {assistant_sensitive_count}/{total_assistant_msgs} ({assistant_sensitive_percentage:.2f}%)\")\n",
    "\n",
    "# Create a bar chart comparing user vs assistant\n",
    "role_data = {\n",
    "    \"Role\": [\"User\", \"Assistant\"],\n",
    "    \"Percentage\": [user_sensitive_percentage, assistant_sensitive_percentage],\n",
    "    \"Count\": [user_sensitive_count, assistant_sensitive_count]\n",
    "}\n",
    "role_df = pd.DataFrame(role_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(role_df[\"Role\"], role_df[\"Percentage\"])\n",
    "plt.xlabel(\"Role\")\n",
    "plt.ylabel(\"Percentage with Sensitive Content\")\n",
    "plt.title(\"Prevalence of Sensitive Content by Role\")\n",
    "for i, v in enumerate(role_df[\"Percentage\"]):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f}%\", ha='center')\n",
    "plt.savefig(f\"{outdir}/role_comparison.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of specific sensitive content types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakdown of sensitive content types:\n",
      "Sexually explicit content (Other): 523 instances\n",
      "Inciting violence, hateful or other harmful behavior (harassment & bullying): 179 instances\n",
      "Sexually explicit content (fictitious person): 154 instances\n",
      "Inciting violence, hateful or other harmful behavior (physical harm): 78 instances\n",
      "Discriminatory practices (Misrepresentation, stereotyping, or inappropriate reference to sensitive attributes): 62 instances\n",
      "Criminal planning or other suspected illegal activity not listed elsewhere: 31 instances\n",
      "Sexually explicit content (Request/discussion of CSAM): 26 instances\n",
      "Sexually explicit content (real person): 20 instances\n",
      "Privacy concerns (Possible identifiable information): 14 instances\n",
      "Weapons & drugs: 11 instances\n",
      "Cyberattacks (enabling/enacting malware, computer viruses, worms, malicious code, ...): 11 instances\n",
      "Privacy concerns (Possible sensitive information): 10 instances\n",
      "Generating defamatory content: 6 instances\n",
      "Inciting violence, hateful or other harmful behavior (self-harm): 5 instances\n",
      "Possible presence of copyrighted, unreferenced material: 3 instances\n",
      "Impersonation attempts: 3 instances\n",
      "Cyberattacks: 1 instances\n",
      "Privacy concerns (Possible sensitive information) (e.g., API keys, passwords, other confidential information): 1 instances\n",
      "Potential violation of external policy / ethics: 1 instances\n",
      "Criminal planning or other suspected illegal activity not listed: 1 instances\n",
      "Other: 1 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhipingzhang/Documents/Phd/projects/NaturalisticData/analysis1/naturalistic-ai/notebooks/../src/helpers/visualisation.py:123: UserWarning: constrained_layout not applied because axes sizes collapsed to zero.  Try making figure larger or Axes decorations smaller.\n",
      "  fig.savefig(output_path, bbox_inches='tight')\n"
     ]
    }
   ],
   "source": [
    "# Analyze types of sensitive content\n",
    "sensitive_types = defaultdict(int)\n",
    "user_sensitive_types = defaultdict(int)\n",
    "assistant_sensitive_types = defaultdict(int)\n",
    "\n",
    "for conv in dataset.data:\n",
    "    for msg in conv.conversation:\n",
    "        if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "            flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "            \n",
    "            # Process flags based on whether they're lists or strings\n",
    "            flags_to_process = []\n",
    "            if isinstance(flag_value, list):\n",
    "                flags_to_process = flag_value\n",
    "            else:\n",
    "                flags_to_process = [flag_value]\n",
    "            \n",
    "            # Count each flag\n",
    "            for flag in flags_to_process:\n",
    "                if flag != \"None\":\n",
    "                    sensitive_types[flag] += 1\n",
    "                    \n",
    "                    # Also track by role\n",
    "                    if msg.role == \"user\":\n",
    "                        user_sensitive_types[flag] += 1\n",
    "                    elif msg.role == \"assistant\":\n",
    "                        assistant_sensitive_types[flag] += 1\n",
    "\n",
    "# Filter out \"None\" category if it exists\n",
    "if \"None\" in sensitive_types:\n",
    "    del sensitive_types[\"None\"]\n",
    "\n",
    "# Plot distribution of sensitive types if any exist\n",
    "if sensitive_types:\n",
    "    fig = barplot_distribution(\n",
    "        {\"Sensitive Content Types\": dict(sensitive_types)}, \n",
    "        normalize=True, \n",
    "        xlabel=\"Content Type\", \n",
    "        ylabel=\"Proportion\", \n",
    "        title=\"Distribution of Sensitive Content Types\",\n",
    "        output_path=f\"{outdir}/content_types.png\", \n",
    "        order=\"descending\"\n",
    "    )\n",
    "\n",
    "    # Print detailed breakdown\n",
    "    print(\"Breakdown of sensitive content types:\")\n",
    "    for content_type, count in sorted(sensitive_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{content_type}: {count} instances\")\n",
    "    \n",
    "    # Compare distribution between user and assistant\n",
    "    if user_sensitive_types and assistant_sensitive_types:\n",
    "        # Create a DataFrame for plotting\n",
    "        types = sorted(set(list(user_sensitive_types.keys()) + list(assistant_sensitive_types.keys())))\n",
    "        comparison_data = []\n",
    "        \n",
    "        for content_type in types:\n",
    "            user_count = user_sensitive_types.get(content_type, 0)\n",
    "            assistant_count = assistant_sensitive_types.get(content_type, 0)\n",
    "            comparison_data.append({\n",
    "                \"Content Type\": content_type,\n",
    "                \"User\": user_count,\n",
    "                \"Assistant\": assistant_count\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Plot side-by-side bars\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        comparison_df.plot(x=\"Content Type\", y=[\"User\", \"Assistant\"], kind=\"bar\", figsize=(14, 8))\n",
    "        plt.title(\"Sensitive Content Types: User vs Assistant\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{outdir}/user_vs_assistant_types.png\")\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"No specific sensitive content types found beyond 'None'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal analysis of sensitive content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution if time data is available\n",
    "if hasattr(dataset.data[0], 'time') and dataset.data[0].time:\n",
    "    # Convert time strings to datetime objects\n",
    "    temporal_data = []\n",
    "    for conv in dataset.data:\n",
    "        if conv.time:\n",
    "            try:\n",
    "                time_obj = datetime.fromisoformat(conv.time.replace('Z', '+00:00'))\n",
    "                has_sensitive = False\n",
    "                \n",
    "                for msg in conv.conversation:\n",
    "                    if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "                        flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "                        if isinstance(flag_value, list):\n",
    "                            if any(flag != \"None\" for flag in flag_value):\n",
    "                                has_sensitive = True\n",
    "                                break\n",
    "                        elif flag_value and flag_value != \"None\":\n",
    "                            has_sensitive = True\n",
    "                            break\n",
    "                \n",
    "                temporal_data.append((time_obj, has_sensitive))\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse time format for conversation {conv.conversation_id}\")\n",
    "    \n",
    "    # Sort by time\n",
    "    temporal_data.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Group by month\n",
    "    monthly_data = defaultdict(lambda: {\"sensitive\": 0, \"total\": 0})\n",
    "    for time_obj, has_sensitive in temporal_data:\n",
    "        month_key = time_obj.strftime(\"%Y-%m\")\n",
    "        monthly_data[month_key][\"total\"] += 1\n",
    "        if has_sensitive:\n",
    "            monthly_data[month_key][\"sensitive\"] += 1\n",
    "    \n",
    "    # Calculate monthly percentages\n",
    "    months = []\n",
    "    percentages = []\n",
    "    \n",
    "    for month, counts in sorted(monthly_data.items()):\n",
    "        months.append(month)\n",
    "        percentage = (counts[\"sensitive\"] / counts[\"total\"]) * 100 if counts[\"total\"] > 0 else 0\n",
    "        percentages.append(percentage)\n",
    "    \n",
    "    # Plot monthly trend\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(months, percentages, marker='o')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Percentage of Conversations with Sensitive Content')\n",
    "    plt.title('Monthly Trend of Sensitive Content')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{outdir}/monthly_trend.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic analysis of sensitive content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breakdown of sensitive content by country:\n",
      "United Arab Emirates: 7/14 (50.00%)\n",
      "Jamaica: 5/13 (38.46%)\n",
      "Estonia: 8/21 (38.10%)\n",
      "Italy: 19/59 (32.20%)\n",
      "TÃ¼rkiye: 14/49 (28.57%)\n",
      "United Kingdom: 36/155 (23.23%)\n",
      "Spain: 4/18 (22.22%)\n",
      "Russia: 131/610 (21.48%)\n",
      "Germany: 29/138 (21.01%)\n",
      "United States: 168/872 (19.27%)\n",
      "Romania: 6/32 (18.75%)\n",
      "The Netherlands: 7/38 (18.42%)\n",
      "Australia: 9/51 (17.65%)\n",
      "Belarus: 4/23 (17.39%)\n",
      "Ukraine: 3/18 (16.67%)\n",
      "Mexico: 3/18 (16.67%)\n",
      "Malaysia: 2/12 (16.67%)\n",
      "Canada: 15/91 (16.48%)\n",
      "France: 20/124 (16.13%)\n",
      "Peru: 2/14 (14.29%)\n",
      "Brazil: 8/58 (13.79%)\n",
      "Kazakhstan: 2/15 (13.33%)\n",
      "Saudi Arabia: 2/17 (11.76%)\n",
      "Indonesia: 2/18 (11.11%)\n",
      "Poland: 3/27 (11.11%)\n",
      "South Korea: 2/19 (10.53%)\n",
      "Hungary: 1/11 (9.09%)\n",
      "Singapore: 4/47 (8.51%)\n",
      "Japan: 7/87 (8.05%)\n",
      "Taiwan: 4/51 (7.84%)\n",
      "Argentina: 1/13 (7.69%)\n",
      "India: 5/70 (7.14%)\n",
      "Vietnam: 2/32 (6.25%)\n",
      "Philippines: 3/57 (5.26%)\n",
      "Morocco: 1/21 (4.76%)\n",
      "New Zealand: 1/25 (4.00%)\n",
      "Iran: 1/26 (3.85%)\n",
      "China: 15/404 (3.71%)\n",
      "Hong Kong: 7/207 (3.38%)\n",
      "Egypt: 2/61 (3.28%)\n",
      "Algeria: 0/16 (0.00%)\n",
      "Israel: 0/15 (0.00%)\n",
      "Greece: 0/10 (0.00%)\n",
      "Uzbekistan: 0/10 (0.00%)\n",
      "DR Congo: 0/49 (0.00%)\n",
      "Chile: 0/12 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze geographical distribution if geography data is available\n",
    "if hasattr(dataset.data[0], 'geography') and dataset.data[0].geography:\n",
    "    # Group by geography\n",
    "    geo_data = defaultdict(lambda: {\"sensitive\": 0, \"total\": 0})\n",
    "    \n",
    "    for conv in dataset.data:\n",
    "        if conv.geography:\n",
    "            # Extract country from geography (format might be \"Country; Region\")\n",
    "            country = conv.geography.split(';')[0].strip() if ';' in conv.geography else conv.geography\n",
    "            \n",
    "            # Check if conversation has sensitive content\n",
    "            has_sensitive = False\n",
    "            for msg in conv.conversation:\n",
    "                if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "                    flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "                    if isinstance(flag_value, list):\n",
    "                        if any(flag != \"None\" for flag in flag_value):\n",
    "                            has_sensitive = True\n",
    "                            break\n",
    "                    elif flag_value and flag_value != \"None\":\n",
    "                        has_sensitive = True\n",
    "                        break\n",
    "            \n",
    "            # Update counts\n",
    "            geo_data[country][\"total\"] += 1\n",
    "            if has_sensitive:\n",
    "                geo_data[country][\"sensitive\"] += 1\n",
    "    \n",
    "    # Calculate percentages and prepare data for plotting\n",
    "    geo_percentages = {}\n",
    "    \n",
    "    for country, counts in geo_data.items():\n",
    "        if counts[\"total\"] >= 10:  # Only include countries with sufficient data\n",
    "            percentage = (counts[\"sensitive\"] / counts[\"total\"]) * 100\n",
    "            geo_percentages[country] = percentage\n",
    "    \n",
    "    # Sort countries by percentage\n",
    "    sorted_countries = sorted(geo_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot geographical distribution\n",
    "    if sorted_countries:\n",
    "        countries, percentages = zip(*sorted_countries[:15])  # Top 15 countries\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.bar(countries, percentages)\n",
    "        plt.xlabel('Country')\n",
    "        plt.ylabel('Percentage of Conversations with Sensitive Content')\n",
    "        plt.title('Sensitive Content by Country (Top 15)')\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{outdir}/geo_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Print detailed breakdown\n",
    "        print(\"\\nBreakdown of sensitive content by country:\")\n",
    "        for country, percentage in sorted_countries:\n",
    "            total = geo_data[country][\"total\"]\n",
    "            sensitive = geo_data[country][\"sensitive\"]\n",
    "            print(f\"{country}: {sensitive}/{total} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breakdown of sensitive content by model:\n",
      "gpt-4-0314: 30/190 (15.79%)\n",
      "gpt-4-1106-preview: 98/635 (15.43%)\n",
      "gpt-3.5-turbo-0613: 280/1864 (15.02%)\n",
      "gpt-3.5-turbo-0301: 103/703 (14.65%)\n",
      "gpt-4-0125-preview: 43/310 (13.87%)\n",
      "gpt-3.5-turbo-0125: 39/298 (13.09%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze by model if model data is available\n",
    "if hasattr(dataset.data[0], 'model') and dataset.data[0].model:\n",
    "    # Group by model\n",
    "    model_data = defaultdict(lambda: {\"sensitive\": 0, \"total\": 0})\n",
    "    \n",
    "    for conv in dataset.data:\n",
    "        if conv.model:\n",
    "            # Check if conversation has sensitive content\n",
    "            has_sensitive = False\n",
    "            for msg in conv.conversation:\n",
    "                if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata:\n",
    "                    flag_value = msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value\n",
    "                    if isinstance(flag_value, list):\n",
    "                        if any(flag != \"None\" for flag in flag_value):\n",
    "                            has_sensitive = True\n",
    "                            break\n",
    "                    elif flag_value and flag_value != \"None\":\n",
    "                        has_sensitive = True\n",
    "                        break\n",
    "            \n",
    "            # Update counts\n",
    "            model_data[conv.model][\"total\"] += 1\n",
    "            if has_sensitive:\n",
    "                model_data[conv.model][\"sensitive\"] += 1\n",
    "    \n",
    "    # Calculate percentages and prepare data for plotting\n",
    "    model_percentages = {}\n",
    "    \n",
    "    for model, counts in model_data.items():\n",
    "        if counts[\"total\"] >= 5:  # Only include models with sufficient data\n",
    "            percentage = (counts[\"sensitive\"] / counts[\"total\"]) * 100\n",
    "            model_percentages[model] = percentage\n",
    "    \n",
    "    # Sort models by percentage\n",
    "    sorted_models = sorted(model_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot model distribution\n",
    "    if sorted_models:\n",
    "        models, percentages = zip(*sorted_models)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(models, percentages)\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Percentage of Conversations with Sensitive Content')\n",
    "        plt.title('Sensitive Content by Model')\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{outdir}/model_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Print detailed breakdown\n",
    "        print(\"\\nBreakdown of sensitive content by model:\")\n",
    "        for model, percentage in sorted_models:\n",
    "            total = model_data[model][\"total\"]\n",
    "            sensitive = model_data[model][\"sensitive\"]\n",
    "            print(f\"{model}: {sensitive}/{total} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "Total conversations: 4000\n",
      "Total messages: 20254\n",
      "Conversations with sensitive content: 593 (14.82%)\n",
      "Messages with sensitive content: 1067 (5.27%)\n",
      "User messages with sensitive content: 1067/10127 (10.54%)\n",
      "Assistant messages with sensitive content: 0/10127 (0.00%)\n",
      "\n",
      "Top sensitive content types:\n",
      "- Sexually explicit content (Other): 523 instances\n",
      "- Inciting violence, hateful or other harmful behavior (harassment & bullying): 179 instances\n",
      "- Sexually explicit content (fictitious person): 154 instances\n",
      "- Inciting violence, hateful or other harmful behavior (physical harm): 78 instances\n",
      "- Discriminatory practices (Misrepresentation, stereotyping, or inappropriate reference to sensitive attributes): 62 instances\n"
     ]
    }
   ],
   "source": [
    "# Calculate some final statistics\n",
    "total_messages = sum(1 for conv in dataset.data for _ in conv.conversation)\n",
    "sensitive_messages = sum(\n",
    "    1 for conv in dataset.data \n",
    "    for msg in conv.conversation \n",
    "    if \"automatic_v0-turn_sensitive_use_flags\" in msg.metadata and (\n",
    "        (isinstance(msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value, list) and \n",
    "         any(flag != \"None\" for flag in msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value)) or\n",
    "        (not isinstance(msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value, list) and \n",
    "         msg.metadata[\"automatic_v0-turn_sensitive_use_flags\"].value != \"None\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total conversations: {total_conv_count}\")\n",
    "print(f\"Total messages: {total_messages}\")\n",
    "print(f\"Conversations with sensitive content: {sensitive_conv_count} ({percentage_sensitive:.2f}%)\")\n",
    "print(f\"Messages with sensitive content: {sensitive_messages} ({(sensitive_messages/total_messages)*100:.2f}%)\")\n",
    "print(f\"User messages with sensitive content: {user_sensitive_count}/{total_user_msgs} ({user_sensitive_percentage:.2f}%)\")\n",
    "print(f\"Assistant messages with sensitive content: {assistant_sensitive_count}/{total_assistant_msgs} ({assistant_sensitive_percentage:.2f}%)\")\n",
    "\n",
    "if sensitive_types:\n",
    "    print(\"\\nTop sensitive content types:\")\n",
    "    for content_type, count in sorted(sensitive_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"- {content_type}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
